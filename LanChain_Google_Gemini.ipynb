{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9aa557-2317-4871-b6cf-95bcb67f7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain.google.genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5211b8ab-b1a0-4dc0-9517-86ca80383737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-google-genai\n",
      "Version: 0.0.11\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\n",
      "Requires: google-generativeai, langchain-core\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1d5ad0-a1d4-4832-af54-a2b91a495f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ecbdaa-b580-42bd-bbde-d5a358eb9d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5a8d2b-9da3-4395-bb4f-756f23cdbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if 'GOOGLE_API_KEY' not in os.environ:\n",
    "    os.environ['GOOGLE_API_KEY'] = getpass.getpass(\"Use your own apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb1bad9-452c-48d0-a7d9-80b9c71ccf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
      "                   'model that supports tuning.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
      "                   'model.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "for model in genai.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef41c1b-9628-4608-ae98-9d24b48f1383",
   "metadata": {},
   "source": [
    "**Integrating Gemini with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7c0ba3-2ef3-4eda-841e-1056b2b84049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the year 2300, life on Mercury has evolved into a subterranean sanctuary. The relentless heat and radiation on the surface have forced humans to seek refuge in vast underground cities, shielded by layers of rock and ice. These subterranean metropolises are marvels of engineering, powered by geothermal energy and illuminated by artificial sunlight. Despite the harsh environment, life on Mercury has flourished. Advanced technologies have enabled the cultivation of crops, providing food and oxygen to the inhabitants. The cities are connected by a network of high-speed rail lines, allowing for efficient transportation and communication. The people of Mercury have adapted to the unique challenges of their planet, forging a resilient and innovative society that thrives beneath the unforgiving surface.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', temperature=0.6)\n",
    "response = llm.invoke('Write a paragraph about life on Mercury in year 2300.')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c612ae9-2097-4ed3-ba85-7a837267c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a content creator. Write as a tweet about Why will AI change the world\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'topic': 'Why will AI change the world', 'text': \"ðŸš€ AI's transformative power is undeniable! It's revolutionizing industries, automating tasks, enhancing decision-making, and unlocking new possibilities. From healthcare to finance, AI is poised to reshape the world as we know it. #AIEvolution #FutureTech\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are a content creator. Write as a tweet about {topic}\")\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "topic = \"Why will AI change the world\"\n",
    "response = chain.invoke(input=topic) # response is a dictionary with two keys, the topic which contains the variable and text that contains the answer of the LLM\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b743fcd-34d0-4560-b4eb-ee652a91c5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why will AI change the world\n",
      "\n",
      "ðŸš€ AI's transformative power is undeniable! It's revolutionizing industries, automating tasks, enhancing decision-making, and unlocking new possibilities. From healthcare to finance, AI is poised to reshape the world as we know it. #AIEvolution #FutureTech\n"
     ]
    }
   ],
   "source": [
    "print(response['topic'])\n",
    "print()\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b75ca-5fff-4b80-b607-99218a35861d",
   "metadata": {},
   "source": [
    "**System Prompt And Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2acd4c2-5b4c-4976-af07-906cf6af3e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NÃ£o'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# SystemMessage is a directive that you provide a personality, it acts as powerful tool to guide the model behaviour within the conversation\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-pro', convert_system_message_to_human=True)\n",
    "output = llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content='Answer only in YES or NO in Portugese'),\n",
    "        HumanMessage(content='Is fish a mammal?')\n",
    "    ]\n",
    ")\n",
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733741a-1930-4750-9d12-7df8df426ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
