{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d694273e-4949-416d-8cf1-50d29f3be2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.7\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\n",
      "Requires: aiohttp, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f85971-d2fb-4d3d-83f5-d8683e3840fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.12.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: \n",
      "Location: D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: langchain-openai\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98f6f4b-3256-486b-a4fb-7a6d981a4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.7\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\n",
      "Requires: aiohttp, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a0f87e-3a62-45ee-957e-f784dfd86982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Python-dotenv\n",
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "#os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d5ce5-9e77-4048-828d-a27c96e67550",
   "metadata": {},
   "source": [
    "# Chat Models: GPT 3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6901f33-82ce-4b29-9553-f3c88cd41070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChatOpenAI in module langchain_openai.chat_models.base:\n",
      "\n",
      "class ChatOpenAI(langchain_core.language_models.chat_models.BaseChatModel)\n",
      " |  ChatOpenAI(*, name: Optional[str] = None, cache: Optional[bool] = None, verbose: bool = None, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, client: Any = None, async_client: Any = None, model: str = 'gpt-3.5-turbo', temperature: float = 0.7, model_kwargs: Dict[str, Any] = None, api_key: Optional[pydantic.v1.types.SecretStr] = None, base_url: Optional[str] = None, organization: Optional[str] = None, openai_proxy: Optional[str] = None, timeout: Union[float, Tuple[float, float], Any, NoneType] = None, max_retries: int = 2, streaming: bool = False, n: int = 1, max_tokens: Optional[int] = None, tiktoken_model_name: Optional[str] = None, default_headers: Optional[Mapping[str, str]] = None, default_query: Optional[Mapping[str, object]] = None, http_client: Optional[Any] = None) -> None\n",
      " |  \n",
      " |  `OpenAI` Chat large language models API.\n",
      " |  \n",
      " |  To use, you should have the\n",
      " |  environment variable ``OPENAI_API_KEY`` set with your API key.\n",
      " |  \n",
      " |  Any parameters that are valid to be passed to the openai.create call can be passed\n",
      " |  in, even if not explicitly saved on this class.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_community.chat_models import ChatOpenAI\n",
      " |          openai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ChatOpenAI\n",
      " |      langchain_core.language_models.chat_models.BaseChatModel\n",
      " |      langchain_core.language_models.base.BaseLanguageModel\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  bind_functions(self, functions: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', function_call: \"Optional[Union[_FunctionCall, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind functions (and other objects) to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI function-calling API.\n",
      " |      \n",
      " |      NOTE: Using bind_tools is recommended instead, as the `functions` and\n",
      " |          `function_call` request parameters are officially marked as deprecated by\n",
      " |          OpenAI.\n",
      " |      \n",
      " |      Args:\n",
      " |          functions: A list of function definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, or callable. Pydantic\n",
      " |              models and callables will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          function_call: Which function to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any).\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  bind_tools(self, tools: 'Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]]', *, tool_choice: \"Optional[Union[dict, str, Literal['auto', 'none']]]\" = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
      " |      Bind tool-like objects to this chat model.\n",
      " |      \n",
      " |      Assumes model is compatible with OpenAI tool-calling API.\n",
      " |      \n",
      " |      Args:\n",
      " |          tools: A list of tool definitions to bind to this chat model.\n",
      " |              Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
      " |              models, callables, and BaseTools will be automatically converted to\n",
      " |              their schema dictionary representation.\n",
      " |          tool_choice: Which tool to require the model to call.\n",
      " |              Must be the name of the single provided function or\n",
      " |              \"auto\" to automatically determine which function to call\n",
      " |              (if any), or a dict of the form:\n",
      " |              {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
      " |          **kwargs: Any additional parameters to pass to the\n",
      " |              :class:`~langchain.runnable.Runnable` constructor.\n",
      " |  \n",
      " |  get_num_tokens_from_messages(self, messages: 'List[BaseMessage]') -> 'int'\n",
      " |      Calculate num tokens for gpt-3.5-turbo and gpt-4 with tiktoken package.\n",
      " |      \n",
      " |      Official documentation: https://github.com/openai/openai-cookbook/blob/\n",
      " |      main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
      " |  \n",
      " |  get_token_ids(self, text: 'str') -> 'List[int]'\n",
      " |      Get the tokens present in the text with tiktoken package.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  build_extra(values: 'Dict[str, Any]') -> 'Dict[str, Any]' from pydantic.v1.main.ModelMetaclass\n",
      " |      Build extra kwargs from additional params that were passed in.\n",
      " |  \n",
      " |  get_lc_namespace() -> 'List[str]' from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |  \n",
      " |  is_lc_serializable() -> 'bool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Return whether this model can be serialized by Langchain.\n",
      " |  \n",
      " |  validate_environment(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Validate that api key and python package exists in environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Config = <class 'langchain_openai.chat_models.base.ChatOpenAI.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'async_client': 'Any', 'client': 'Any', 'default_he...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'async_client': True, 'callback_manager': True, ...\n",
      " |  \n",
      " |  __fields__ = {'async_client': ModelField(name='async_client', type=Opt...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(False, <function BaseChatModel.raise_depr...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function ChatOpenAI.build_extra>]\n",
      " |  \n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...Non...\n",
      " |  \n",
      " |  __validators__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __call__(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  async agenerate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async agenerate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Asynchronously pass a sequence of prompts and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async apredict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  call_as_llm(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  dict(self, **kwargs: 'Any') -> 'Dict'\n",
      " |      Return a dictionary of the LLM.\n",
      " |  \n",
      " |  generate(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          messages: List of list of messages.\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  generate_prompt(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
      " |      Pass a sequence of prompts to the model and return model generations.\n",
      " |      \n",
      " |      This method should make use of batched calls for models that expose a batched\n",
      " |      API.\n",
      " |      \n",
      " |      Use this method when you want to:\n",
      " |          1. take advantage of batched calls,\n",
      " |          2. need more output from the model than just the top generated value,\n",
      " |          3. are building chains that are agnostic to the underlying language model\n",
      " |              type (e.g., pure text completion models vs chat models).\n",
      " |      \n",
      " |      Args:\n",
      " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
      " |              converted to match the format of any language model (string for pure\n",
      " |              text generation models and BaseMessages for chat models).\n",
      " |          stop: Stop words to use when generating. Model output is cut off at the\n",
      " |              first occurrence of any of these substrings.\n",
      " |          callbacks: Callbacks to pass through. Used for executing additional\n",
      " |              functionality, such as logging or streaming, throughout generation.\n",
      " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
      " |              to the model provider API call.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An LLMResult, which contains a list of candidate Generations for each input\n",
      " |              prompt and additional model provider-specific output.\n",
      " |  \n",
      " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |  \n",
      " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  predict_messages(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
      " |      [*Deprecated*][*Deprecated*] \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 0.1.7\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  OutputType\n",
      " |      Get the output type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.language_models.base.BaseLanguageMode...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  get_num_tokens(self, text: 'str') -> 'int'\n",
      " |      Get the number of tokens present in the text.\n",
      " |      \n",
      " |      Useful for checking if an input will fit in a model's context window.\n",
      " |      \n",
      " |      Args:\n",
      " |          text: The string input to tokenize.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The integer number of tokens in the text.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
      " |  \n",
      " |  InputType\n",
      " |      Get the input type for this runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __init__(self, **kwargs: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json(self) -> Union[langchain_core.load.serializable.SerializedConstructor, langchain_core.load.serializable.SerializedNotImplemented]\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*]  Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator ove StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      * ``event``: str - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      * ``name``: str - The name of the runnable that generated the event.\n",
      " |      * ``run_id``: str - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      * ``tags``: Optional[List[str]] - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      * ``metadata``: Optional[Dict[str, Any]] - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      * ``data``: Dict[str, Any]\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      |----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      ```python\n",
      " |      def format_docs(docs: List[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.[*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator ove StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      * ``event``: str - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      * ``name``: str - The name of the runnable that generated the event.\n",
      " |      * ``run_id``: str - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      * ``tags``: Optional[List[str]] - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      * ``metadata``: Optional[Dict[str, Any]] - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      * ``data``: Dict[str, Any]\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      |----------------------|------------------|---------------------------------|-----------------------------------------------|-------------------------------------------------|\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      \n",
      " |      Here are declarations associated with the events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      ```python\n",
      " |      def format_docs(docs: List[Document]) -> str:\n",
      " |          '''Format the docs.'''\n",
      " |          return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |      format_docs = RunnableLambda(format_docs)\n",
      " |      ```\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tool\n",
      " |      def some_tool(x: int, y: str) -> dict:\n",
      " |          '''Some_tool.'''\n",
      " |          return {\"x\": x, \"y\": y}\n",
      " |      ```\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      ```python\n",
      " |      template = ChatPromptTemplate.from_messages(\n",
      " |          [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |      ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |  \n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |  \n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChatOpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57839ae-f7c3-4147-be54-5899eb5dc0c7",
   "metadata": {},
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(\"Write a Resume bullet short paragraph of 2 lines covering the following ->Constructed and Utilized the Sakila Movie Database to analyze movie rental trends and customer behaviors, executing complex SQL queries in MySQL Workbench\", temperature=0.99)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3a4e36-b77a-40a9-a3d0-47dc12d1d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing a leveraged Large Language Model, specifically FLAN-T5, for the task of dialogue summarization involved investigating the effects of auto tokenization and the utilization of AutoModelForSeq2SeqLM. The model was fine-tuned using advanced techniques like LORA through generative configuration adjustments to enhance its overall performance. Additionally, innovative prompt engineering methods were developed to improve dialogue interpretation and summarization capabilities.\n"
     ]
    }
   ],
   "source": [
    "# System message corresponds to system message open AI chatgpt\n",
    "# AI MEssage corresponds to assistant message\n",
    "# Human Message corresponds to users message or prompt\n",
    "from langchain.schema import(\n",
    "SystemMessage,\n",
    "AIMessage,\n",
    "HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Large Language Model Engineer and respond only in LLM Technical Jargon.\"),\n",
    "                  HumanMessage(content=\"Rephrase <- Leveraged LLM, notably FLAN-T5, for dialogue summarization, exploring auto tokenization and AutoModelForSeq2SeqLM's impact; fine-tuned with techniques such as LORA via generative configuration adjustments, enhancing model performance; pioneered prompt engineering for improved dialogue interpretation and summarization\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3b8023-8f6e-41b7-88ad-9056f779bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current downfall in the US market can be attributed to a variety of factors. Some of the key reasons include concerns about rising inflation, potential interest rate hikes by the Federal Reserve, uncertainty surrounding the ongoing trade disputes, and geopolitical tensions. Additionally, the recent surge in Covid-19 cases and the impact of the Delta variant have also contributed to market volatility and investor unease. Overall, these factors have combined to create a challenging environment for the US stock market and have led to recent declines in major indices.\n"
     ]
    }
   ],
   "source": [
    "# System message corresponds to system message open AI chatgpt\n",
    "# AI MEssage corresponds to assistant message\n",
    "# Human Message corresponds to users message or prompt\n",
    "from langchain.schema import(\n",
    "SystemMessage,\n",
    "AIMessage,\n",
    "HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a Finance Advisor and respond only in English.\"),\n",
    "                  HumanMessage(content=\"Explain the current US market downfall\")\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7248b0-5f49-4ad4-8761-73465d4c054f",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84d0a3e1-4fd3-45d4-a4b8-cb65685449b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "I lost my job, now I'm feeling low\n",
      "The economy's down, there's nowhere to go\n",
      "I've been sending out resumes, day after day\n",
      "But all I hear is \"sorry, not today\"\n",
      "\n",
      "Chorus:\n",
      "Unemployment, it's a hard pill to swallow\n",
      "Feeling lost and empty, like there's no tomorrow\n",
      "I'm just another statistic in the US market\n",
      "Hoping and praying for a new start\n",
      "\n",
      "Verse 2:\n",
      "I used to have a steady paycheck and a sense of pride\n",
      "But now I'm struggling just to survive\n",
      "The bills keep piling up, the stress is weighing me down\n",
      "I never thought I'd be in this position, feeling like a clown\n",
      "\n",
      "Chorus:\n",
      "Unemployment, it's a hard pill to swallow\n",
      "Feeling lost and empty, like there's no tomorrow\n",
      "I'm just another statistic in the US market\n",
      "Hoping and praying for a new start\n",
      "\n",
      "Bridge:\n",
      "I try to stay positive, keep my head held high\n",
      "But it's hard when all I hear is \"goodbye\"\n",
      "I know I'm not alone in this fight\n",
      "But it's hard to see the light\n",
      "\n",
      "Chorus:\n",
      "Unemployment, it's a hard pill to swallow\n",
      "Feeling lost and empty, like there's no tomorrow\n",
      "I'm just another statistic in the US market\n",
      "Hoping and praying for a new start\n",
      "\n",
      "Outro:\n",
      "I won't give up, I'll keep pushing through\n",
      "I know there's a job out there for me, it's true\n",
      "I'll keep on searching, keep on trying\n",
      "Because I know I'm worth more than just crying.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a Unemployment Song on the current US market'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ab0244-d841-4191-b47d-945518bb79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "I wake up in the morning, feeling so alone\n",
      "No job to go to, no income to bring home\n",
      "I've been searching high and low, sending out my resume\n",
      "But the job market's tough, there's just no work today\n",
      "\n",
      "(Chorus)\n",
      "Unemployment, oh unemployment\n",
      "It's a struggle just to make ends meet\n",
      "Unemployment, oh unemployment\n",
      "I'm feeling defeated, facing defeat\n",
      "\n",
      "(Verse 2)\n",
      "I used to have a steady job, a paycheck every week\n",
      "But now I'm standing in line, just trying to make ends meet\n",
      "The bills keep piling up, the rent is overdue\n",
      "I'm feeling so hopeless, don't know what to do\n",
      "\n",
      "(Chorus)\n",
      "Unemployment, oh unemployment\n",
      "It's a struggle just to make ends meet\n",
      "Unemployment, oh unemployment\n",
      "I'm feeling defeated, facing defeat\n",
      "\n",
      "(Bridge)\n",
      "I never thought I'd be in this position\n",
      "Just trying to survive, just trying to make a living\n",
      "But the economy's in shambles, the job market's a mess\n",
      "I'm just another statistic, another one under duress\n",
      "\n",
      "(Chorus)\n",
      "Unemployment, oh unemployment\n",
      "It's a struggle just to make ends meet\n",
      "Unemployment, oh unemployment\n",
      "I'm feeling defeated, facing defeat\n",
      "\n",
      "(Outro)\n",
      "I'll keep on fighting, I won't give up the fight\n",
      "I'll keep on searching, until I see the light\n",
      "But for now, I'll keep on singing this song\n",
      "About the struggles of unemployment, and how it feels so wrong."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end = '', flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8151fa9-fae0-40b4-a182-61e9e38f343c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You are an experienced Data Scientist.\\nWrite a few sentences about the following analytics \"predictive\" in Retail Sector. '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ''' You are an experienced Data Scientist.\n",
    "Write a few sentences about the following analytics \"{analytics}\" in {domain}. '''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(analytics='predictive', domain='Retail Sector')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842dc949-9938-40ee-bf25-420727306918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ''' You are an experienced LLM Machine Learning Expert write in {LLM}.\n",
    "Rephrase this resume bullet point in {prompt}. '''\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(prompt=\"Leveraged LLM, notably FLAN-T5, for dialogue summarization, exploring auto tokenization and AutoModelForSeq2SeqLM's impact; fine-tuned with techniques such as LORA via generative configuration adjustments, enhancing model performance; pioneered prompt engineering for improved dialogue interpretation and summarization\", LLM='Machine Learning LLM writer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12a0db1f-64a0-416f-a6f7-dd6082c7b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilized LLM, specifically FLAN-T5, for dialogue summarization, delving into auto tokenization and leveraging AutoModelForSeq2SeqLM for improved results; fine-tuned with LORA techniques through generative configuration adjustments to enhance model performance; spearheaded prompt engineering to enhance dialogue interpretation and summarization.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04fafeae-bfa1-4074-aff1-29b4dcdf7bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilized Leveraged LLM, specifically FLAN-T5, for dialogue summarization, delving into auto tokenization and AutoModelForSeq2SeqLM's influence; fine-tuned using methods like LORA through generative configuration tweaks to boost model efficacy; spearheaded prompt engineering to enhance dialogue comprehension and summarization.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635869b-2e6d-4c6b-abb7-19acccb6a649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0e4264-e0f3-4984-b696-7d855fcebb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 10 countries in Asia by population.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "# HumanMessagePromptTemplate is used to create dynamic promnpts\n",
    "# SystemMessage is to create formatting for the prompt\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(n='10', area='Asia')\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fe51fe6-3586-4fa8-b843-76c8ccc1f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\"rank\": 1, \"country\": \"China\", \"population\": \"1,439,323,776\"},\n",
      "        {\"rank\": 2, \"country\": \"India\", \"population\": \"1,380,004,385\"},\n",
      "        {\"rank\": 3, \"country\": \"Indonesia\", \"population\": \"276,361,783\"},\n",
      "        {\"rank\": 4, \"country\": \"Pakistan\", \"population\": \"225,199,937\"},\n",
      "        {\"rank\": 5, \"country\": \"Bangladesh\", \"population\": \"166,303,498\"},\n",
      "        {\"rank\": 6, \"country\": \"Japan\", \"population\": \"126,476,461\"},\n",
      "        {\"rank\": 7, \"country\": \"Philippines\", \"population\": \"113,266,317\"},\n",
      "        {\"rank\": 8, \"country\": \"Vietnam\", \"population\": \"98,168,833\"},\n",
      "        {\"rank\": 9, \"country\": \"Iran\", \"population\": \"84,923,419\"},\n",
      "        {\"rank\": 10, \"country\": \"Turkey\", \"population\": \"84,339,067\"}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66f8b58-3739-4866-8963-d3e225e6cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experience virologost.\n",
    "Write a few sentences about the following vrus \"{virus}\" in {l}. '''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'HSV', 'l': 'Spanish'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e43c88d-b845-4370-9fe0-6864674f5d0f",
   "metadata": {},
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ccd0bce-911d-44b5-a4d3-803d8b4062f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'HSV', 'l': 'Spanish', 'text': 'El virus del herpes simple (HSV) es un patógeno humano común que causa infecciones en la piel y las mucosas. Se divide en dos tipos, HSV-1 y HSV-2, que son responsables del herpes labial y genital respectivamente. Este virus puede permanecer latente en el cuerpo y reactivarse en momentos de estrés o debilidad del sistema inmunitario, causando síntomas como ampollas, fiebre y malestar general. La prevención y el tratamiento adecuado son fundamentales para controlar la infección por HSV y prevenir su transmisión a otras personas.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a278c92b-2e88-4550-aba4-4d29cfafb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Below is a Python function that implements linear regression using the method of least squares:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    n = len(X)\n",
      "    \n",
      "    # Calculate the mean of X and y\n",
      "    mean_X = np.mean(X)\n",
      "    mean_y = np.mean(y)\n",
      "    \n",
      "    # Calculate the slope (m) and y-intercept (b) of the line\n",
      "    numerator = np.sum((X - mean_X) * (y - mean_y))\n",
      "    denominator = np.sum((X - mean_X)**2)\n",
      "    m = numerator / denominator\n",
      "    b = mean_y - m * mean_X\n",
      "    \n",
      "    return m, b\n",
      "\n",
      "# Example usage\n",
      "X = np.array([1, 2, 3, 4, 5])\n",
      "y = np.array([2, 4, 5, 4, 5])\n",
      "\n",
      "slope, intercept = linear_regression(X, y)\n",
      "\n",
      "print(\"Slope:\", slope)\n",
      "print(\"Y-intercept:\", intercept)\n",
      "```\n",
      "\n",
      "This function takes in two numpy arrays X and y, which represent the independent and dependent variables, respectively. It then calculates the slope (m) and y-intercept (b) of the best fit line using the least squares method and returns them. \n",
      "\n",
      "You can test this function by passing in your own dataset X and y and see how well it fits the data points.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis Python function, named `linear_regression`, implements the method of least squares to calculate the slope and y-intercept of a linear regression line that best fits a given dataset. Here's a breakdown of how the function works:\n",
      "\n",
      "1. The function takes two numpy arrays as input: `X` and `y`, which represent the independent and dependent variables respectively.\n",
      "\n",
      "2. It calculates the mean of the `X` values and the mean of the `y` values using the `np.mean()` function.\n",
      "\n",
      "3. The numerator is calculated as the sum of the product of the differences between each `X` value and the mean `X` value and each `y` value and the mean `y` value.\n",
      "\n",
      "4. The denominator is calculated as the sum of the squared differences between each `X` value and the mean `X` value.\n",
      "\n",
      "5. The slope `m` is then calculated by dividing the numerator by the denominator.\n",
      "\n",
      "6. The y-intercept `b` is computed using the formula `b = mean_y - m * mean_X`. This means that the line fitting the data points is given by `y = m * x + b`.\n",
      "\n",
      "7. Finally, the function returns the calculated slope and y-intercept values.\n",
      "\n",
      "In the example provided, the function is tested with the `X` values `[1, 2, 3, 4, 5]` and `y` values `[2, 4, 5, 4, 5]`. The calculated slope and y-intercept are printed to the console.\n",
      "\n",
      "To test the function with your own dataset, you can pass in different arrays `X` and `y` containing the relevant data points and observe how well the calculated line fits the data in terms of slope and y-intercept. By comparing these values to the actual trends in your data, you can evaluate the effectiveness of the linear regression model in approximating the relationship between the variables.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chains\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template = \"You are an experienced data scientist and Python programmer. Write a function that implements the concept of {concept}.\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt = prompt_template1)\n",
    "\n",
    "llm2 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1.2)\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template = \"Given the Python function {function}, describe it as detailed as possible.\"\n",
    ")\n",
    "# For prescision give temperature parameter = 0.4~0.6 and for creativity 1.1~1.4\n",
    "chain2 = LLMChain(llm=llm2, prompt = prompt_template2)\n",
    "# Second chain output goes as first chain input\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be4d575f-582f-4d03-9857-22e3caae9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'linear regression', 'output': \"This Python function, named `linear_regression`, implements the method of least squares to calculate the slope and y-intercept of a linear regression line that best fits a given dataset. Here's a breakdown of how the function works:\\n\\n1. The function takes two numpy arrays as input: `X` and `y`, which represent the independent and dependent variables respectively.\\n\\n2. It calculates the mean of the `X` values and the mean of the `y` values using the `np.mean()` function.\\n\\n3. The numerator is calculated as the sum of the product of the differences between each `X` value and the mean `X` value and each `y` value and the mean `y` value.\\n\\n4. The denominator is calculated as the sum of the squared differences between each `X` value and the mean `X` value.\\n\\n5. The slope `m` is then calculated by dividing the numerator by the denominator.\\n\\n6. The y-intercept `b` is computed using the formula `b = mean_y - m * mean_X`. This means that the line fitting the data points is given by `y = m * x + b`.\\n\\n7. Finally, the function returns the calculated slope and y-intercept values.\\n\\nIn the example provided, the function is tested with the `X` values `[1, 2, 3, 4, 5]` and `y` values `[2, 4, 5, 4, 5]`. The calculated slope and y-intercept are printed to the console.\\n\\nTo test the function with your own dataset, you can pass in different arrays `X` and `y` containing the relevant data points and observe how well the calculated line fits the data in terms of slope and y-intercept. By comparing these values to the actual trends in your data, you can evaluate the effectiveness of the linear regression model in approximating the relationship between the variables.\"}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b17d9-c449-426d-bb5c-86de9e8eebcb",
   "metadata": {},
   "source": [
    "## LangChain Agents in Action: Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c7fd456-f222-41f9-bb12-77bfbc56b02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0ac5a9a-ec8f-4928-8ec7-f75ac8d6b849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1,100) if n% 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a686b5ca-1073-476f-94cc-eda3cca3629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can first calculate the factorial of 12 and then find the square root of that result. Finally, we can display the square root with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe need to import the math module to access the square root function.\n",
      "Action: Python_REPL\n",
      "Action Input: math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe have calculated the factorial of 12, now we can find the square root of this result.\n",
      "Action: Python_REPL\n",
      "Action Input: math.sqrt(math.factorial(12))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe have found the square root of the factorial of 12, now we can display it with 4 decimal points.\n",
      "Action: Python_REPL\n",
      "Action Input: print(\"{:.4f}\".format(math.sqrt(math.factorial(12))))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm = llm,\n",
    "    tool = PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "agent_executor.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a35fd1-f6c7-4f0a-9c75-c74e6f33a016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can calculate the result of 5.1 ** 7.3 using Python's exponentiation operator.\n",
      "Action: Python_REPL\n",
      "Action Input: 5.1 ** 7.3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 10487.865616551346\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a781ca-b7dc-4d01-be06-f8b9c3ec4e8f",
   "metadata": {},
   "source": [
    "5.1**7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e17bd089-7eee-4600-99a3-82c995bf4616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146306.05007233328"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.1**7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb2ebe0c-842e-4ea2-bf97-4e6751fc0098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the answer to 5.1 ** 7.3?\n"
     ]
    }
   ],
   "source": [
    "print(response['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80ec2266-802d-46c6-95fa-9586e22ad7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10487.865616551346\n"
     ]
    }
   ],
   "source": [
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea344656-9d98-4932-ae68-c32ec4b856cb",
   "metadata": {},
   "source": [
    "## LangChain Tools: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6129941b-64b6-454a-9c1b-426ce2264dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "973d33f9-7704-422a-9c12-099fab6364ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But there are many lesser-known Freddie Mercury facts that reveal hidden depths to the life and work of the man born Farrokh Bulsara in Zanzibar, on September 5, 1946. Here are 15 surprising... Freddie Mercury was born Farrokh Bulsara in Stone Town, in the British protectorate of Zanzibar (now part of Tanzania), on September 5, 1946. His first big challenge was to come to terms with... Although he was born on the East African island of Zanzibar on September 5, 1946, his parents were both Indian; they were Parsees from Gujarat. Mercury would often make note of his Persian roots, but the Parsees had been driven from Persia a thousand years before. They landed on the Gujarat coast and populated its greatest city, Bombay. Table of Contents Freddie Mercury was born on September 5, 1946. Freddie Mercury, whose real name was Farrokh Bulsara, was born in Zanzibar, Tanzania. He later became the lead vocalist of the legendary rock band Queen. Freddie Mercury had a four-octave vocal range. Freddie Mercury (born Farrokh Bulsara; 5 September 1946 - 24 November 1991) was a British singer, songwriter, record producer, and lead vocalist of the rock band Queen. Regarded as one of the greatest lead singers in the history of rock music, he was known for his flamboyant stage persona and four-octave vocal range. Another One Bites The Dust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\\curl_cffi\\aio.py:192: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "output = search.invoke('Where was Freddie Mercury born?')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dcbd378-b7af-4b74-bc11-ec9095fe0ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c554ec11-a0e4-41c9-aa0d-b884749105b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f398729-a962-4590-aa57-c951c7804201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: From his earliest songs with Queen, Mercury built a reputation as one of the most popular songwriters of his generation, and the best Freddie Mercury songs reveal an artist committed to..., title: Best Freddie Mercury Songs: 20 Essential Solo And Queen Tracks, link: https://www.udiscovermusic.com/stories/best-freddie-mercury-songs/], [snippet: Everybody knows that Freddie Mercury was a gifted songwriter, a breathtakingly original performer, and frontman of one of the greatest bands of all time, Queen. But there are many..., title: Freddie Mercury Facts: Things You Never Knew About Queen's Frontman, link: https://www.udiscovermusic.com/stories/freddie-mercury-facts-you-need-to-know/], [snippet: Ex-fiancée explains decision Who is Mary Austin? Meet the woman Freddie Mercury asked to marry Freddie Mercury facts: Queen singer's age, teeth, real name, relationships and more explained Freddie Mercury in private: 20 rarely seen photos of the star behind closed doors, title: Freddie Mercury: Mary Austin breaks silence on relationship with Queen ..., link: https://www.smoothradio.com/artists/freddie-mercury/mary-austin-interview-relationship/], [snippet: May 31, 2023 Leer en español It's one of rock's best-known and strangest songs: a six-minute radio hit that starts out as a piano ballad, becomes a high-pitched opera, then tumbles into a..., title: Was Queen's 'Bohemian Rhapsody' First ... - The New York Times, link: https://www.nytimes.com/2023/05/31/arts/music/queen-freddie-mercury-bohemian-rhapsody.html]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\\curl_cffi\\aio.py:192: UserWarning: Curlm alread closed! quitting from process_data\n",
      "  warnings.warn(\"Curlm alread closed! quitting from process_data\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchResults()\n",
    "output = search.run('Freddie Mercury and Queen')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d82c490c-4ea7-4299-9c00-ec062422a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region = 'de-de', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('Berlin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01e37e72-a723-43d4-98f8-fe2f5b2594fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Berlin [bɛr'li:n] ist die Hauptstadt und ein Land der Bundesrepublik Deutschland. Die Großstadt ist mit rund 3,8 Millionen Einwohnern die bevölkerungsreichste und mit 892 Quadratkilometern die flächengrößte Gemeinde Deutschlands sowie die bevölkerungsreichste Stadt der Europäischen Union., title: Berlin - Wikipedia, link: https://de.wikipedia.org/wiki/Berlin], [snippet: Einfach zurechtfinden: Mit unserer übersichtlichen Liste der Sehenswürdigkeiten in Berlin planst du deinen Städtetrip ohne Umwege., title: Berlin Sehenswürdigkeiten: Top-25-Liste mit Tipps & Bezirken, link: https://www.voucherwonderland.com/reisemagazin/top-20-sehenswuerdigkeiten-in-berlin/], [snippet: Die dokumentierte Geschichte der Stadt Berlin begann im Hochmittelalter mit der Gründung von zwei Handelsorten. Urkundlich erstmals erwähnt wurde Berlin im Jahr 1244, das benachbarte Kölln bereits 1237. Archäologische Funde legen nahe, dass beide Orte Jahrhunderte vorher besiedelt waren., title: Geschichte Berlins - Wikipedia, link: https://de.wikipedia.org/wiki/Geschichte_Berlins], [snippet: Damit du einen Überblick über die Lage der genannten Ausflugsziele und Aktivitäten in Berlin bekommst, haben wir dir alle Highlights in einer interaktiven Karte abgespeichert. Paddeln im Seenland Oder-Spree. Das sind unsere Top 15 Unternehmungen, nachfolgend findest du alle Aktivitäten für Berlin., title: Was kann man in Berlin machen? TOP 25 Aktivitäten & Unternehmungen!, link: https://www.traveloptimizer.de/ausflugsziele-berlin/]\n"
     ]
    }
   ],
   "source": [
    "print(output\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2382054-3ef4-4206-8872-1b83895aed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9d05a31-b2e8-4e1d-98b5-9e0fc832c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'snipper: (.*?), title: (.*?), link: (.*)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05605f1-427d-4b84-a3c9-604190e50b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1360b-6f71-4bc1-81ef-b583ed866334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
