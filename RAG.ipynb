{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b265394a-af09-4193-95e7-fa3794e54ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac12622d-a424-424e-aa2d-3f5fbaa7a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41b59f3-841e-46eb-8698-21fbe4ac70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee68710-bb6e-472b-9984-d271be6598c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Use your own key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0762436e-5d45-424b-9b35-b1ea24e8c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614f6953-5241-4af0-9285-a33e12948903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c9347c-024b-460d-8c49-719fc1403cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        \n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data\n",
    "    \n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64308ac-e304-4ead-ac47-945f89e7f46d",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84c3488-f58e-4f89-a8fd-8a1f5f64978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existings_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca0a126-4b2a-48fb-9d9e-2a77779159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ...')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end = '')\n",
    "        pc.delete.index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df825782-b95c-4a3a-bb9f-35c4b9438260",
   "metadata": {},
   "source": [
    "## Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f47dcb5-8d93-45d7-9d04-ab9112983a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading us_constitution.pdf\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "be\n",
      "composed\n",
      "of\n",
      "Members\n",
      "chosen\n",
      "every\n",
      "second\n",
      "Y ear\n",
      "by\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "several\n",
      "States,\n",
      "and\n",
      "the\n",
      "Electors\n",
      "in\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "the\n",
      "Qualifications\n",
      "requisite\n",
      "for\n",
      "Electors\n",
      "of\n",
      "the\n",
      "most\n",
      "numerous\n",
      "Branch\n",
      "of\n",
      "the\n",
      "State\n",
      "Legislature.\n",
      "No\n",
      "Person\n",
      "shall\n",
      "be\n",
      "a\n",
      "Representative\n",
      "who\n",
      "shall\n",
      "not\n",
      "have\n",
      "attained\n",
      "to\n",
      "the\n",
      "Age\n",
      "of\n",
      "twenty\n",
      "five\n",
      "Y ears,\n",
      "and\n",
      "been\n",
      "seven\n",
      "Y ears\n",
      "a\n",
      "Citizen\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "who\n",
      "shall\n",
      "not,\n",
      "when\n",
      "elected,\n",
      "be\n",
      "an\n",
      "Inhabitant\n",
      "of\n",
      "that\n",
      "State\n",
      "in\n",
      "which\n",
      "he\n",
      "shall\n",
      "be\n",
      "chosen.\n",
      "Representatives\n",
      "and\n",
      "direct\n",
      "T axes\n",
      "shall\n",
      "be\n",
      "apportioned\n",
      "among\n",
      "the\n",
      "several\n",
      "States\n",
      "which\n",
      "may\n",
      "be\n",
      "included\n",
      "within\n",
      "this\n",
      "Union,\n",
      "according\n",
      "to\n",
      "their\n",
      "respective\n",
      "Numbers,\n",
      "which\n",
      "shall\n",
      "be\n",
      "determined\n",
      "by\n",
      "adding\n",
      "to\n",
      "the\n",
      "whole\n",
      "Number\n",
      "of\n",
      "free\n",
      "Persons,\n",
      "including\n",
      "those\n",
      "bound\n",
      "to\n",
      "Service\n",
      "for\n",
      "a\n",
      "T erm\n",
      "of\n",
      "Y ears,\n",
      "and\n",
      "excluding\n",
      "Indians\n",
      "not\n",
      "taxed,\n",
      "three\n",
      "fifths\n",
      "of\n",
      "all\n",
      "other\n",
      "Persons.\n",
      "The\n",
      "actual\n",
      "Enumeration\n",
      "shall\n",
      "be\n",
      "made\n",
      "within\n",
      "three\n",
      "Y ears\n",
      "after\n",
      "the\n",
      "first\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "within\n",
      "every\n",
      "subsequent\n",
      "T erm\n",
      "of\n",
      "ten\n",
      "Y ears,\n",
      "in\n",
      "such\n",
      "Manner\n",
      "as\n",
      "they\n",
      "shall\n",
      "by\n",
      "Law\n",
      "direct.The\n",
      "number\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "not\n",
      "exceed\n",
      "one\n",
      "for\n",
      "every\n",
      "thirty\n",
      "Thousand,\n",
      "but\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "at\n",
      "Least\n",
      "one\n",
      "Representative;\n",
      "and\n",
      "until\n",
      "such\n",
      "enumeration\n",
      "shall\n",
      "be\n",
      "made,\n",
      "the\n",
      "State\n",
      "of\n",
      "New\n",
      "Hampshire\n",
      "shall\n",
      "be\n",
      "entitled\n",
      "to\n",
      "chuse\n",
      "three,\n",
      "Massachusetts\n",
      "eight,\n",
      "Rhode-Island\n",
      "and\n",
      "Providence\n",
      "Plantations\n",
      "one,\n",
      "Connecticut\n",
      "five,\n",
      "New-Y ork\n",
      "six,\n",
      "New\n",
      "Jersey\n",
      "four ,\n",
      "Pennsylvania\n",
      "eight,\n",
      "Delaware\n",
      "one,\n",
      "{'source': 'us_constitution.pdf', 'page': 10}\n",
      "You have 41 pages in your data\n",
      "There are 1137 Characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('us_constitution.pdf')\n",
    "print(data[1].page_content)\n",
    "print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} Characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b0b9dd-2c12-4adb-b399-d53ddc8230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Letter\n",
      "\n",
      "To Whom so ever it may concern\n",
      "\n",
      "February 2024\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I trust this letter finds you well. As I explore new career opportunities, I am eager to express my interest in potential data related roles within your esteemed organization. As a current Master International student at the University of Texas at Dallas, pursuing Information Technology and Management I have completed the Graduate Certification’s in Applied Machine Learning, Business Intelligence, and Data Mining. Moreover, I’m simultaneously working as a Graduate Teaching Assistant.\n",
      "\n",
      "Having worked at Anglo Eastern Shipping Management in the dual capacity of Marine Engineer and Data Analytics Specialist, I have led in significant cost savings, notably achieving $1.5M/year through insightful analysis. Proficient in Python, SQL, and AWS technologies, I executed a project that resulted in a 30% reduction in breakdowns ($300K savings). I am enthusiastic about the prospect of leveraging my unique skill set to drive operational excellence and contribute to the success of your team.\n",
      "\n",
      "As an ardent advocate for Data Science and a forward-thinking visionary in Deep Learning, my hands-on experience encompasses Computer Vision models, specializing in Object Detection Algorithms such as YOLO, SSD, and Faster R-CNN, utilizing PyTorch and TensorFlow. Beyond this, my expertise extends into Applied Machine Learning, where I've honed my skills in developing Categorical models with Scikit-learn Logistic regression, employing GridSearchCV for Hyperparameter Tuning. I conducted a comparative analysis introducing the LightGBM model, optimized with Optuna for enhanced space/memory utilization and accelerated results. Proficient in H2O models, I adeptly utilize Exploratory Data Analysis techniques with Pandas in Python and R Programming. My data analysis process integrates statistical analysis and hypothesis testing as fundamental components.\n",
      "\n",
      "I recently earned the Microsoft Data Scientist Azure Associate certification and obtained certifications in IBM Data Science and Deeplearning.AI TensorFlow Developer. Actively engaged in a Deep Learning project on Object Detection using Sage Maker and AWS Step Functions, I'm utilizing Computer Vision models in Fast AI and torch vision for transfer learning. Incorporating wandb, Resnet model architecture, and Efficient Net with Inception, my proficiency extends to working with Microsoft Azure Machine Learning Studio and AWS Sage Maker.\n",
      "\n",
      "In my coursework, I've delved into Accounting and Financial Management, gaining proficiency in deciphering financial reports, and assessing a company's outreach while identifying areas for improvement. My substantial experience in the operations domain enhances my skill set, providing a comprehensive understanding of business processes. Leveraging my strong background in the business domain, I possess a natural aptitude for swiftly learning new concepts and thrive in collaborative team environments. I am sincerely grateful for your time in contemplating my application and am eager to further discuss how my skills align with the needs of your team. I look forward to the opportunity of hearing from you soon.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('cover_lt.docx')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be162db0-34b6-4b1a-977e-7f366ef73f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI's API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It  can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads. To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code an\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('GPT-4')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d8bdc8-a18d-4d22-9101-62ace35a27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e58ac07-6f38-46dd-bc47-c20e90029ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "== Capabilities ==\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3737a120-fe33-4345-ac43-e07da667acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d97c0dc-c415-47c4-bd85-cd26ca066822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbb6f0-a1b8-4bbb-acb7-09f11e861267",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82a0538c-539f-4ff0-a3a0-14779d9a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will retrieve the most relevant chunk of text from our vector database then\n",
    "# we will feed those chunks to LLM to get the final answer\n",
    "\n",
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', searc_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9522803c-3a51-441f-aaf2-ec87e1d2046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Gre\\UTD\\Courses\\Spring_II\\MIS6382\\Softwares\\Python\\Python311\\class\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the whole document about?', 'result': \"I don't have access to the content of the document you are referring to. If you can provide more context or specific information, I may be able to help you better.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5218fcc-11b0-45aa-97cd-43aa6a46c09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting ... bye bye !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i+1\n",
    "    if q.lower() in ['quit','exit']:\n",
    "        print(\"Quitting ... bye bye !\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f67cdb3-1c65-4e74-8884-43aedace3e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86a6498-42cf-4aba-9535-5bece5815689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'ro')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85b723c2-ca09-4e99-a3c6-4b44f5b0952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Ce este ChatGPT?', 'result': 'ChatGPT este un model de limbaj generativ bazat pe tehnologia GPT-3 (Generative Pre-trained Transformer 3), dezvoltat de OpenAI. Acesta poate genera texte care sunt coerente și rezonabile, având capacitatea de a răspunde la întrebări și de a purta conversații în mod natural.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"Ce este ChatGPT?\" \n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7531d5a2-4da7-4460-b955-c1e7502b2654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Cand a fost lansat GPT4?', 'result': 'GPT-4 a fost lansat pe 14 martie 2023 și este disponibil doar pentru abonații plătitori.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"Cand a fost lansat GPT4?\" \n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f15711-4a68-4501-9640-1b639705472a",
   "metadata": {},
   "source": [
    "## It gives good memory but it lacks memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510f785-8482-4faf-b19a-5291d7e5b1db",
   "metadata": {},
   "source": [
    "## Chroma as Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e139548-d04f-426f-862c-dcc151271bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "352bc11f-24fb-4ee1-94f5-40c3ff317be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persistent_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiating an embedding model to convert text to numerical representations\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persistent_directory)\n",
    "    return vector_store\n",
    "\n",
    "# Function to load the existing embeddings from disk to a vector store object\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "      # Instantiating an embedding model to convert text to numerical representations\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vector_store\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9659a0d-aca0-43cc-ab00-83153d99e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "# Lets load a PDF File\n",
    "data = load_document('rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b092e73d-8c7d-4694-8309-aac124ad3ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is vertex AI Search?', 'result': 'Vertex AI Search is a feature that offers customizable answers, search tuning, vector search, grounding, and compliance updates for enterprises. It also includes new generative AI capabilities and enterprise-ready features.'}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3bdbda4-c205-4163-a139-56ed84e8ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How many pairs of questions and answers had the StackOverflow dataset?', 'result': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4c287-6240-4c05-b1f8-5a13f257857e",
   "metadata": {},
   "source": [
    "### As we can see from the pdf it answers correctly\n",
    "### However there is a drawback to this, if I ask a follow up question, it will not have access to the previous chat history and will respond that it doesn't know the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d81d3a41-bab4-4976-aaa7-7a8ad214db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Multiply that number by 2. ', 'result': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "# For example,\n",
    "q = 'Multiply that number by 2. '\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbefbe-aabc-4408-8ffb-16089f4ebd87",
   "metadata": {},
   "source": [
    "### As we can see it could not answer, next we add Memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd678ab-0267-47f7-9ba6-a02523281330",
   "metadata": {},
   "source": [
    "### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba668ef-1e39-4a35-af9e-eeb864bc3477",
   "metadata": {},
   "source": [
    "## Common requirement for RAG is support for follow up questions.\n",
    "## Follow up questions can contain references to past chat history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2748217b-027b-4f4f-a1ad-1baf597f8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# This chain is used to have a conversation based on the retrieved documents\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# This acts as a buffer for storing conversation\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature = 0)\n",
    "# Retriever is a crucial component that helps LLM's find and access relevant information\n",
    "# it does this by searching the relevant data and retrieving the information\n",
    "# Here the retriever will search by similarity and will retrieve the top k most similar chunks\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "\n",
    "# Creating a memory object that will be passed to the conversational retrieval chain as an argument\n",
    "# the memory will be automatically updated with the questions and answers\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Creating the Conversational Retrieval Chains\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff', # use all of the text from the documents\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23a8dd22-13f4-4729-81fa-9cd2c0306071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "564962f5-9c97-4d76-8030-5de86d4195e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "# Using the same rag powered google search pdf\n",
    "# Loading document\n",
    "data = load_document('rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e33e42db-00eb-4817-856f-c9a88abe07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'), AIMessage(content='The StackOverflow dataset had 8 million pairs of questions and answers.')], 'answer': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "#result = {}\n",
    "q = \"How many pairs of questions and answers had the StackOverflow dataset?\"\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88bb90e6-dd7d-4db7-82ef-54dec139ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The StackOverflow dataset had 8 million pairs of questions and answers.\n"
     ]
    }
   ],
   "source": [
    "# if only want answer\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cf1540a-32dc-410d-9c67-f21153ea4c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Follow Up Input: Multiple that number by 10.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "Human: What is the result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "q = 'Multiple that number by 10.'\n",
    "result = ask_question(q, crc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71f135f5-c922-427b-acc2-d78c0061ed01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bffcd2-82a7-4e0a-8128-f087ba5d57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "#!pip install streamlit_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf34dc2-87fd-4a6a-9016-92377351644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import streamlit as st\n",
    "from streamlit_chat import message\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import  PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "#from langchain_community.chat_message_histories import StreamLitChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3c507-da75-4770-b6d9-120c033ec06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from langchain_community.chat_message_histories import (\n",
    "    StreamlitChatMessageHistory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61ebd5-1438-4cfa-9fbe-8936ebdbe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load_dotenv(find_dotenv())\n",
    "\n",
    "template = \"\"\"You are an AI chatbot having a conversation with a human\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "AI: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variable=[\"history\",\"human_input\"], template=template)\n",
    "\n",
    "msgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", chat_memory=msgs)\n",
    "\n",
    "def load_chain():\n",
    "    llm = ChatOpenAI(temperature = 0)\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "    return llm_chain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6ed911f-ed43-432a-bf91-507c2817a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: How many pairs of questions and answers had the StackOverflow dataset?\n",
      "Assistant: The StackOverflow dataset had 8 million pairs of questions and answers.\n",
      "Human: Multiple that number by 10.\n",
      "Assistant: The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.\n",
      "Follow Up Input: Divide the result by 80\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "Human: What is the result of dividing 80 million pairs by 80?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The result of dividing 80 million pairs by 80 is 1 million.\n"
     ]
    }
   ],
   "source": [
    "q = 'Divide the result by 80'\n",
    "result = ask_question(q, crc)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5959c1b-1e71-4f7c-ac4f-d1446c3578aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Divide the result by 80',\n",
       " 'chat_history': [HumanMessage(content='How many pairs of questions and answers had the StackOverflow dataset?'),\n",
       "  AIMessage(content='The StackOverflow dataset had 8 million pairs of questions and answers.'),\n",
       "  HumanMessage(content='Multiple that number by 10.'),\n",
       "  AIMessage(content='The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.'),\n",
       "  HumanMessage(content='Divide the result by 80'),\n",
       "  AIMessage(content='The result of dividing 80 million pairs by 80 is 1 million.')],\n",
       " 'answer': 'The result of dividing 80 million pairs by 80 is 1 million.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f70e8345-63b4-4a12-9fda-2f3a691d4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How many pairs of questions and answers had the StackOverflow dataset?'\n",
      "content='The StackOverflow dataset had 8 million pairs of questions and answers.'\n",
      "content='Multiple that number by 10.'\n",
      "content='The result of multiplying the number of pairs of questions and answers in the StackOverflow dataset by 10 would be 80 million pairs.'\n",
      "content='Divide the result by 80'\n",
      "content='The result of dividing 80 million pairs by 80 is 1 million.'\n"
     ]
    }
   ],
   "source": [
    "# To check the chat history\n",
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df802ef-2544-4dfe-8f5d-b0ae81530797",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "51997d2b-05d9-49b8-9f04-3ec512d19942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using prompt engineering technique to ask questions in a specific ways\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_message=True)\n",
    "\n",
    "# Define the template for system prompt, this sets the general behavior of the LLM\n",
    "system_template = r'''' \n",
    "Use the following pieces of context to answer the user's question.\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "# Now the template for the user prompt \n",
    "\n",
    "user_template = '''\n",
    "Question: ```{question}```\n",
    "Chat History: ```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc=ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1573d70a-ea26-4567-a9a0-f6d7f33229db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['chat_history', 'context', 'question'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"' \\nUse the following pieces of context to answer the user's question.\\n---------------\\nContext: ```{context}```\\n\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='\\nQuestion: ```{question}```\\nChat History: ```{chat_history}```\\n'))]\n"
     ]
    }
   ],
   "source": [
    "print(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c3cdac5-4de0-4379-803f-7adaa85e25bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ' \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-```\n",
      "\n",
      "Human: \n",
      "Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\n",
      "Chat History: ``````\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': '', 'answer': 'Based on the provided context, the StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fff25de8-6142-4aa6-a4db-11a2fbaa78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ' \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------\n",
      "Context: ```simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-\n",
      "\n",
      "simple similarity search was highly e\u0000ective because the dataset had 8\n",
      "million pairs of questions and answers. However, datasets do not\n",
      "usually contain pre-existing question-and-answer or query-and-```\n",
      "\n",
      "Human: \n",
      "Question: ```How many pairs of questions and answers had the StackOverflow dataset?```\n",
      "Chat History: ``````\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'How many pairs of questions and answers had the StackOverflow dataset?', 'chat_history': '', 'answer': 'Based on the provided context, the StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a052328-e1c4-4d38-b5fd-f8cc13727833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: ' \n",
      "Use the following pieces of context to answer the user's question.\n",
      "---------------\n",
      "Context: ```house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is\n",
      "\n",
      "house AI processors back in 2013 — the Tensor Processing Unit (TPU).\n",
      "TPUs are speci\u0000cally tailored to provide the underlying power needed\n",
      "to suppo\u0000 machine learning and AI workloads, but their genesis is```\n",
      "\n",
      "Human: \n",
      "Question: ```When was elon musk born?```\n",
      "Chat History: ``````\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'When was elon musk born?', 'chat_history': '', 'answer': \"I'm sorry, but based on the provided context, I cannot determine Elon Musk's birthdate. Would you like me to look up Elon Musk's birthdate for you?\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'When was elon musk born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10314dd-814d-4392-9ad9-8bfeb3beb222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
