{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b265394a-af09-4193-95e7-fa3794e54ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac12622d-a424-424e-aa2d-3f5fbaa7a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d41b59f3-841e-46eb-8698-21fbe4ac70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee68710-bb6e-472b-9984-d271be6598c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Use your own key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0762436e-5d45-424b-9b35-b1ea24e8c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "614f6953-5241-4af0-9285-a33e12948903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73c9347c-024b-460d-8c49-719fc1403cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        \n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data\n",
    "    \n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64308ac-e304-4ead-ac47-945f89e7f46d",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d84c3488-f58e-4f89-a8fd-8a1f5f64978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existings_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ca0a126-4b2a-48fb-9d9e-2a77779159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ...')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end = '')\n",
    "        pc.delete.index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df825782-b95c-4a3a-bb9f-35c4b9438260",
   "metadata": {},
   "source": [
    "## Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f47dcb5-8d93-45d7-9d04-ab9112983a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading us_constitution.pdf\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "be\n",
      "composed\n",
      "of\n",
      "Members\n",
      "chosen\n",
      "every\n",
      "second\n",
      "Y ear\n",
      "by\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "several\n",
      "States,\n",
      "and\n",
      "the\n",
      "Electors\n",
      "in\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "the\n",
      "Qualifications\n",
      "requisite\n",
      "for\n",
      "Electors\n",
      "of\n",
      "the\n",
      "most\n",
      "numerous\n",
      "Branch\n",
      "of\n",
      "the\n",
      "State\n",
      "Legislature.\n",
      "No\n",
      "Person\n",
      "shall\n",
      "be\n",
      "a\n",
      "Representative\n",
      "who\n",
      "shall\n",
      "not\n",
      "have\n",
      "attained\n",
      "to\n",
      "the\n",
      "Age\n",
      "of\n",
      "twenty\n",
      "five\n",
      "Y ears,\n",
      "and\n",
      "been\n",
      "seven\n",
      "Y ears\n",
      "a\n",
      "Citizen\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "who\n",
      "shall\n",
      "not,\n",
      "when\n",
      "elected,\n",
      "be\n",
      "an\n",
      "Inhabitant\n",
      "of\n",
      "that\n",
      "State\n",
      "in\n",
      "which\n",
      "he\n",
      "shall\n",
      "be\n",
      "chosen.\n",
      "Representatives\n",
      "and\n",
      "direct\n",
      "T axes\n",
      "shall\n",
      "be\n",
      "apportioned\n",
      "among\n",
      "the\n",
      "several\n",
      "States\n",
      "which\n",
      "may\n",
      "be\n",
      "included\n",
      "within\n",
      "this\n",
      "Union,\n",
      "according\n",
      "to\n",
      "their\n",
      "respective\n",
      "Numbers,\n",
      "which\n",
      "shall\n",
      "be\n",
      "determined\n",
      "by\n",
      "adding\n",
      "to\n",
      "the\n",
      "whole\n",
      "Number\n",
      "of\n",
      "free\n",
      "Persons,\n",
      "including\n",
      "those\n",
      "bound\n",
      "to\n",
      "Service\n",
      "for\n",
      "a\n",
      "T erm\n",
      "of\n",
      "Y ears,\n",
      "and\n",
      "excluding\n",
      "Indians\n",
      "not\n",
      "taxed,\n",
      "three\n",
      "fifths\n",
      "of\n",
      "all\n",
      "other\n",
      "Persons.\n",
      "The\n",
      "actual\n",
      "Enumeration\n",
      "shall\n",
      "be\n",
      "made\n",
      "within\n",
      "three\n",
      "Y ears\n",
      "after\n",
      "the\n",
      "first\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "within\n",
      "every\n",
      "subsequent\n",
      "T erm\n",
      "of\n",
      "ten\n",
      "Y ears,\n",
      "in\n",
      "such\n",
      "Manner\n",
      "as\n",
      "they\n",
      "shall\n",
      "by\n",
      "Law\n",
      "direct.The\n",
      "number\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "not\n",
      "exceed\n",
      "one\n",
      "for\n",
      "every\n",
      "thirty\n",
      "Thousand,\n",
      "but\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "at\n",
      "Least\n",
      "one\n",
      "Representative;\n",
      "and\n",
      "until\n",
      "such\n",
      "enumeration\n",
      "shall\n",
      "be\n",
      "made,\n",
      "the\n",
      "State\n",
      "of\n",
      "New\n",
      "Hampshire\n",
      "shall\n",
      "be\n",
      "entitled\n",
      "to\n",
      "chuse\n",
      "three,\n",
      "Massachusetts\n",
      "eight,\n",
      "Rhode-Island\n",
      "and\n",
      "Providence\n",
      "Plantations\n",
      "one,\n",
      "Connecticut\n",
      "five,\n",
      "New-Y ork\n",
      "six,\n",
      "New\n",
      "Jersey\n",
      "four ,\n",
      "Pennsylvania\n",
      "eight,\n",
      "Delaware\n",
      "one,\n",
      "{'source': 'us_constitution.pdf', 'page': 10}\n",
      "You have 41 pages in your data\n",
      "There are 1137 Characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('us_constitution.pdf')\n",
    "print(data[1].page_content)\n",
    "print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} Characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78b0b9dd-2c12-4adb-b399-d53ddc8230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Letter\n",
      "\n",
      "To Whom so ever it may concern\n",
      "\n",
      "February 2024\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I trust this letter finds you well. As I explore new career opportunities, I am eager to express my interest in potential data related roles within your esteemed organization. As a current Master International student at the University of Texas at Dallas, pursuing Information Technology and Management I have completed the Graduate Certification’s in Applied Machine Learning, Business Intelligence, and Data Mining. Moreover, I’m simultaneously working as a Graduate Teaching Assistant.\n",
      "\n",
      "Having worked at Anglo Eastern Shipping Management in the dual capacity of Marine Engineer and Data Analytics Specialist, I have led in significant cost savings, notably achieving $1.5M/year through insightful analysis. Proficient in Python, SQL, and AWS technologies, I executed a project that resulted in a 30% reduction in breakdowns ($300K savings). I am enthusiastic about the prospect of leveraging my unique skill set to drive operational excellence and contribute to the success of your team.\n",
      "\n",
      "As an ardent advocate for Data Science and a forward-thinking visionary in Deep Learning, my hands-on experience encompasses Computer Vision models, specializing in Object Detection Algorithms such as YOLO, SSD, and Faster R-CNN, utilizing PyTorch and TensorFlow. Beyond this, my expertise extends into Applied Machine Learning, where I've honed my skills in developing Categorical models with Scikit-learn Logistic regression, employing GridSearchCV for Hyperparameter Tuning. I conducted a comparative analysis introducing the LightGBM model, optimized with Optuna for enhanced space/memory utilization and accelerated results. Proficient in H2O models, I adeptly utilize Exploratory Data Analysis techniques with Pandas in Python and R Programming. My data analysis process integrates statistical analysis and hypothesis testing as fundamental components.\n",
      "\n",
      "I recently earned the Microsoft Data Scientist Azure Associate certification and obtained certifications in IBM Data Science and Deeplearning.AI TensorFlow Developer. Actively engaged in a Deep Learning project on Object Detection using Sage Maker and AWS Step Functions, I'm utilizing Computer Vision models in Fast AI and torch vision for transfer learning. Incorporating wandb, Resnet model architecture, and Efficient Net with Inception, my proficiency extends to working with Microsoft Azure Machine Learning Studio and AWS Sage Maker.\n",
      "\n",
      "In my coursework, I've delved into Accounting and Financial Management, gaining proficiency in deciphering financial reports, and assessing a company's outreach while identifying areas for improvement. My substantial experience in the operations domain enhances my skill set, providing a comprehensive understanding of business processes. Leveraging my strong background in the business domain, I possess a natural aptitude for swiftly learning new concepts and thrive in collaborative team environments. I am sincerely grateful for your time in contemplating my application and am eager to further discuss how my skills align with the needs of your team. I look forward to the opportunity of hearing from you soon.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('cover_lt.docx')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be162db0-34b6-4b1a-977e-7f366ef73f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI's API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It  can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads. To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code an\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('GPT-4')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00d8bdc8-a18d-4d22-9101-62ace35a27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e58ac07-6f38-46dd-bc47-c20e90029ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "== Capabilities ==\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3737a120-fe33-4345-ac43-e07da667acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d97c0dc-c415-47c4-bd85-cd26ca066822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbb6f0-a1b8-4bbb-acb7-09f11e861267",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82a0538c-539f-4ff0-a3a0-14779d9a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will retrieve the most relevant chunk of text from our vector database then\n",
    "# we will feed those chunks to LLM to get the final answer\n",
    "\n",
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', searc_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9522803c-3a51-441f-aaf2-ec87e1d2046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the whole document about?', 'result': \"I'm sorry, but you haven't provided any specific document or topic to give you a specific answer. If you can provide more context or details, I'd be happy to help summarize the document for you.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5218fcc-11b0-45aa-97cd-43aa6a46c09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting ... bye bye !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i+1\n",
    "    if q.lower() in ['quit','exit']:\n",
    "        print(\"Quitting ... bye bye !\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'\\nAnswer: {answer}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f67cdb3-1c65-4e74-8884-43aedace3e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e86a6498-42cf-4aba-9535-5bece5815689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chatgpt and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'ro')\n",
    "chunks = chunk_data(data)\n",
    "index_name = 'chatgpt'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85b723c2-ca09-4e99-a3c6-4b44f5b0952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Ce este ChatGPT?', 'result': 'ChatGPT (Chat Generative Pre-trained Transformer) este un chatbot lansat de OpenAI în noiembrie 2022. Acesta este un membru al familiei de modele de limbaj generative pre-antrenate și a fost inițial bazat pe GPT-3.5. A fost creat pentru a oferi conversații articulate și răspunsuri detaliate în diferite domenii. O versiune bazată pe GPT-4 a fost lansată pe 14 martie 2023, disponibilă doar pentru abonații plătitori.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"Ce este ChatGPT?\" \n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7531d5a2-4da7-4460-b955-c1e7502b2654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Cand a fost lansat GPT4?', 'result': 'GPT-4 a fost lansat pe 14 martie 2023.'}\n"
     ]
    }
   ],
   "source": [
    "q = \"Cand a fost lansat GPT4?\" \n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f15711-4a68-4501-9640-1b639705472a",
   "metadata": {},
   "source": [
    "## It gives good memory but it lacks memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2510f785-8482-4faf-b19a-5291d7e5b1db",
   "metadata": {},
   "source": [
    "## Chroma as Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e139548-d04f-426f-862c-dcc151271bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "352bc11f-24fb-4ee1-94f5-40c3ff317be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persistent_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiating an embedding model to convert text to numerical representations\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persistent_directory)\n",
    "    return vector_store\n",
    "\n",
    "# Function to load the existing embeddings from disk to a vector store object\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "      # Instantiating an embedding model to convert text to numerical representations\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    return vector_store\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b9659a0d-aca0-43cc-ab00-83153d99e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading rag_powered_by_google_search.pdf\n"
     ]
    }
   ],
   "source": [
    "# Lets load a PDF File\n",
    "data = load_document('rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b092e73d-8c7d-4694-8309-aac124ad3ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is vertex AI Search?', 'result': \"Vertex AI Search is a feature within Google's Vertex AI platform that offers new generative AI capabilities and enterprise-ready features. It provides customizable answers, search tuning, vector search, grounding, and compliance updates specifically designed for enterprises. It aims to enhance search capabilities and provide powerful search features to businesses using the Vertex AI platform.\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is vertex AI Search?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a3bdbda4-c205-4163-a139-56ed84e8ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How many pairs of questions and answers had the StackOverflow dataset?', 'result': 'The StackOverflow dataset had 8 million pairs of questions and answers.'}\n"
     ]
    }
   ],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4c287-6240-4c05-b1f8-5a13f257857e",
   "metadata": {},
   "source": [
    "### As we can see from the pdf it answers correctly\n",
    "### However there is a drawback to this, if I ask a follow up question, it will not have access to the previous chat history and will respond that it doesn't know the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d81d3a41-bab4-4976-aaa7-7a8ad214db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Multiply that number by 2. ', 'result': \"I'm sorry, but I cannot provide a specific number to multiply by 2 based on the given context. If you have a specific number or query in mind, please provide it, and I would be happy to help further.\"}\n"
     ]
    }
   ],
   "source": [
    "# For example,\n",
    "q = 'Multiply that number by 2. '\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cbefbe-aabc-4408-8ffb-16089f4ebd87",
   "metadata": {},
   "source": [
    "### As we can see it could not answer, next we add Memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd678ab-0267-47f7-9ba6-a02523281330",
   "metadata": {},
   "source": [
    "### Adding memory to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade901d9-dc1b-46d8-8625-039f51fcd30c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
