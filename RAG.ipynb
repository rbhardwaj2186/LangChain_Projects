{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b265394a-af09-4193-95e7-fa3794e54ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac12622d-a424-424e-aa2d-3f5fbaa7a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41b59f3-841e-46eb-8698-21fbe4ac70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cee68710-bb6e-472b-9984-d271be6598c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Use your own key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0762436e-5d45-424b-9b35-b1ea24e8c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614f6953-5241-4af0-9285-a33e12948903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c9347c-024b-460d-8c49-719fc1403cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        \n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()\n",
    "    return data\n",
    "    \n",
    "# wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lag=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64308ac-e304-4ead-ac47-945f89e7f46d",
   "metadata": {},
   "source": [
    "## Embedding and Uploading to a Vector Database(Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d84c3488-f58e-4f89-a8fd-8a1f5f64978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existings_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "        return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ca0a126-4b2a-48fb-9d9e-2a77779159f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ...')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end = '')\n",
    "        pc.delete.index(index_name)\n",
    "        print('Ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df825782-b95c-4a3a-bb9f-35c4b9438260",
   "metadata": {},
   "source": [
    "## Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f47dcb5-8d93-45d7-9d04-ab9112983a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading us_constitution.pdf\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "be\n",
      "composed\n",
      "of\n",
      "Members\n",
      "chosen\n",
      "every\n",
      "second\n",
      "Y ear\n",
      "by\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "several\n",
      "States,\n",
      "and\n",
      "the\n",
      "Electors\n",
      "in\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "the\n",
      "Qualifications\n",
      "requisite\n",
      "for\n",
      "Electors\n",
      "of\n",
      "the\n",
      "most\n",
      "numerous\n",
      "Branch\n",
      "of\n",
      "the\n",
      "State\n",
      "Legislature.\n",
      "No\n",
      "Person\n",
      "shall\n",
      "be\n",
      "a\n",
      "Representative\n",
      "who\n",
      "shall\n",
      "not\n",
      "have\n",
      "attained\n",
      "to\n",
      "the\n",
      "Age\n",
      "of\n",
      "twenty\n",
      "five\n",
      "Y ears,\n",
      "and\n",
      "been\n",
      "seven\n",
      "Y ears\n",
      "a\n",
      "Citizen\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "who\n",
      "shall\n",
      "not,\n",
      "when\n",
      "elected,\n",
      "be\n",
      "an\n",
      "Inhabitant\n",
      "of\n",
      "that\n",
      "State\n",
      "in\n",
      "which\n",
      "he\n",
      "shall\n",
      "be\n",
      "chosen.\n",
      "Representatives\n",
      "and\n",
      "direct\n",
      "T axes\n",
      "shall\n",
      "be\n",
      "apportioned\n",
      "among\n",
      "the\n",
      "several\n",
      "States\n",
      "which\n",
      "may\n",
      "be\n",
      "included\n",
      "within\n",
      "this\n",
      "Union,\n",
      "according\n",
      "to\n",
      "their\n",
      "respective\n",
      "Numbers,\n",
      "which\n",
      "shall\n",
      "be\n",
      "determined\n",
      "by\n",
      "adding\n",
      "to\n",
      "the\n",
      "whole\n",
      "Number\n",
      "of\n",
      "free\n",
      "Persons,\n",
      "including\n",
      "those\n",
      "bound\n",
      "to\n",
      "Service\n",
      "for\n",
      "a\n",
      "T erm\n",
      "of\n",
      "Y ears,\n",
      "and\n",
      "excluding\n",
      "Indians\n",
      "not\n",
      "taxed,\n",
      "three\n",
      "fifths\n",
      "of\n",
      "all\n",
      "other\n",
      "Persons.\n",
      "The\n",
      "actual\n",
      "Enumeration\n",
      "shall\n",
      "be\n",
      "made\n",
      "within\n",
      "three\n",
      "Y ears\n",
      "after\n",
      "the\n",
      "first\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "and\n",
      "within\n",
      "every\n",
      "subsequent\n",
      "T erm\n",
      "of\n",
      "ten\n",
      "Y ears,\n",
      "in\n",
      "such\n",
      "Manner\n",
      "as\n",
      "they\n",
      "shall\n",
      "by\n",
      "Law\n",
      "direct.The\n",
      "number\n",
      "of\n",
      "Representatives\n",
      "shall\n",
      "not\n",
      "exceed\n",
      "one\n",
      "for\n",
      "every\n",
      "thirty\n",
      "Thousand,\n",
      "but\n",
      "each\n",
      "State\n",
      "shall\n",
      "have\n",
      "at\n",
      "Least\n",
      "one\n",
      "Representative;\n",
      "and\n",
      "until\n",
      "such\n",
      "enumeration\n",
      "shall\n",
      "be\n",
      "made,\n",
      "the\n",
      "State\n",
      "of\n",
      "New\n",
      "Hampshire\n",
      "shall\n",
      "be\n",
      "entitled\n",
      "to\n",
      "chuse\n",
      "three,\n",
      "Massachusetts\n",
      "eight,\n",
      "Rhode-Island\n",
      "and\n",
      "Providence\n",
      "Plantations\n",
      "one,\n",
      "Connecticut\n",
      "five,\n",
      "New-Y ork\n",
      "six,\n",
      "New\n",
      "Jersey\n",
      "four ,\n",
      "Pennsylvania\n",
      "eight,\n",
      "Delaware\n",
      "one,\n",
      "{'source': 'us_constitution.pdf', 'page': 10}\n",
      "You have 41 pages in your data\n",
      "There are 1137 Characters in the page\n"
     ]
    }
   ],
   "source": [
    "data = load_document('us_constitution.pdf')\n",
    "print(data[1].page_content)\n",
    "print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} Characters in the page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78b0b9dd-2c12-4adb-b399-d53ddc8230ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cover Letter\n",
      "\n",
      "To Whom so ever it may concern\n",
      "\n",
      "February 2024\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I trust this letter finds you well. As I explore new career opportunities, I am eager to express my interest in potential data related roles within your esteemed organization. As a current Master International student at the University of Texas at Dallas, pursuing Information Technology and Management I have completed the Graduate Certification’s in Applied Machine Learning, Business Intelligence, and Data Mining. Moreover, I’m simultaneously working as a Graduate Teaching Assistant.\n",
      "\n",
      "Having worked at Anglo Eastern Shipping Management in the dual capacity of Marine Engineer and Data Analytics Specialist, I have led in significant cost savings, notably achieving $1.5M/year through insightful analysis. Proficient in Python, SQL, and AWS technologies, I executed a project that resulted in a 30% reduction in breakdowns ($300K savings). I am enthusiastic about the prospect of leveraging my unique skill set to drive operational excellence and contribute to the success of your team.\n",
      "\n",
      "As an ardent advocate for Data Science and a forward-thinking visionary in Deep Learning, my hands-on experience encompasses Computer Vision models, specializing in Object Detection Algorithms such as YOLO, SSD, and Faster R-CNN, utilizing PyTorch and TensorFlow. Beyond this, my expertise extends into Applied Machine Learning, where I've honed my skills in developing Categorical models with Scikit-learn Logistic regression, employing GridSearchCV for Hyperparameter Tuning. I conducted a comparative analysis introducing the LightGBM model, optimized with Optuna for enhanced space/memory utilization and accelerated results. Proficient in H2O models, I adeptly utilize Exploratory Data Analysis techniques with Pandas in Python and R Programming. My data analysis process integrates statistical analysis and hypothesis testing as fundamental components.\n",
      "\n",
      "I recently earned the Microsoft Data Scientist Azure Associate certification and obtained certifications in IBM Data Science and Deeplearning.AI TensorFlow Developer. Actively engaged in a Deep Learning project on Object Detection using Sage Maker and AWS Step Functions, I'm utilizing Computer Vision models in Fast AI and torch vision for transfer learning. Incorporating wandb, Resnet model architecture, and Efficient Net with Inception, my proficiency extends to working with Microsoft Azure Machine Learning Studio and AWS Sage Maker.\n",
      "\n",
      "In my coursework, I've delved into Accounting and Financial Management, gaining proficiency in deciphering financial reports, and assessing a company's outreach while identifying areas for improvement. My substantial experience in the operations domain enhances my skill set, providing a comprehensive understanding of business processes. Leveraging my strong background in the business domain, I possess a natural aptitude for swiftly learning new concepts and thrive in collaborative team environments. I am sincerely grateful for your time in contemplating my application and am eager to further discuss how my skills align with the needs of your team. I look forward to the opportunity of hearing from you soon.\n"
     ]
    }
   ],
   "source": [
    "data = load_document('cover_lt.docx')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be162db0-34b6-4b1a-977e-7f366ef73f4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WikipediaLoader.__init__() got an unexpected keyword argument 'lag'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_wikipedia\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGPT-4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36mload_from_wikipedia\u001b[1;34m(query, lang, load_max_docs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_wikipedia\u001b[39m(query, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, load_max_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[1;32m---> 23\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_max_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_max_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mTypeError\u001b[0m: WikipediaLoader.__init__() got an unexpected keyword argument 'lag'"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('GPT-4')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00d8bdc8-a18d-4d22-9101-62ace35a27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e58ac07-6f38-46dd-bc47-c20e90029ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Step Functions, I'm utilizing Computer Vision models in Fast AI and torch vision for transfer learning. Incorporating wandb, Resnet model architecture, and Efficient Net with Inception, my proficiency extends to working with Microsoft Azure Machine\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3737a120-fe33-4345-ac43-e07da667acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d97c0dc-c415-47c4-bd85-cd26ca066822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index askadocument and embeddings ...Ok\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askadocument'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbb6f0-a1b8-4bbb-acb7-09f11e861267",
   "metadata": {},
   "source": [
    "## Asking and Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82a0538c-539f-4ff0-a3a0-14779d9a0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will retrieve the most relevant chunk of text from our vector database then\n",
    "# we will feed those chunks to LLM to get the final answer\n",
    "\n",
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', searc_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9522803c-3a51-441f-aaf2-ec87e1d2046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the whole document about?', 'result': \"I don't have access to the whole document that you are referring to. Could you please provide more specific information or context so I can assist you better?\"}\n"
     ]
    }
   ],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5218fcc-11b0-45aa-97cd-43aa6a46c09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
